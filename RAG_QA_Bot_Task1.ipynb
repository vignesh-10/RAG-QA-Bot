{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5188fad",
   "metadata": {},
   "source": [
    "# Task 1 - RAG Model for QA Bot\n",
    "This Colab notebook implements a Retrieval-Augmented Generation (RAG) model for a QA bot, leveraging the OpenAI API and Pinecone DB. The QA bot answers questions based on a set of business documents.\n",
    "\n",
    "## Key Components:\n",
    "- **Text Preprocessing**: Removes special characters, lemmatizes words, and filters out stopwords.\n",
    "- **Pinecone Indexing**: Documents are tokenized into sentences, embedded, and added to Pinecone for retrieval.\n",
    "- **Retrieval**: Retrieves relevant documents based on query embedding similarity.\n",
    "- **Generation**: Uses GPT-3.5-turbo to generate answers from the retrieved context.\n",
    "- **Gradio Interface**: Provides a user interface to interact with the bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pinecone-client nltk gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pinecone\n",
    "import nltk\n",
    "from typing import List, Dict, Any\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import logging\n",
    "import gradio as gr\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up API keys (you'll need to set these in Colab)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = \"your-pinecone-environment\"  # e.g., 'us-west1-gcp'\n",
    "\n",
    "# Initialize Pinecone\n",
    "try:\n",
    "    pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "except Exception as e:\n",
    "    logger.error(f'Failed to initialize Pinecone: {str(e)}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or connect to an index\n",
    "index_name = \"business-qa-index\"\n",
    "try:\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        pinecone.create_index(index_name, dimension=1536)  # OpenAI embeddings are 1536 dimensions\n",
    "    index = pinecone.Index(index_name)\n",
    "except Exception as e:\n",
    "    logger.error(f'Failed to create or connect to Pinecone index: {str(e)}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0500d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    '''Preprocess the text by removing special characters, lemmatizing, and removing stopwords.'''\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and digits\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word.lower()) for word in text.split()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7271805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> List[float]:\n",
    "    '''Get embedding for a given text using OpenAI's API.'''\n",
    "    try:\n",
    "        response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
    "        return response['data'][0]['embedding']\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to get embedding: {str(e)}')\n",
    "        raise\n",
    "\n",
    "def add_documents_to_index(documents: List[str]):\n",
    "    '''Add documents to the Pinecone index.'''\n",
    "    try:\n",
    "        for i, doc in enumerate(documents):\n",
    "            preprocessed_doc = preprocess_text(doc)\n",
    "            sentences = sent_tokenize(doc)\n",
    "            for j, sentence in enumerate(sentences):\n",
    "                embedding = get_embedding(sentence)\n",
    "                index.upsert(vectors=[(f\"{i}-{j}\", embedding, {\"text\": sentence, \"preprocessed\": preprocess_text(sentence)})])\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to add documents to index: {str(e)}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b61a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_docs(query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "    '''Retrieve the k most relevant documents for a given query.'''\n",
    "    try:\n",
    "        query_embedding = get_embedding(query)\n",
    "        results = index.query(query_embedding, top_k=k, include_metadata=True)\n",
    "        return [{\"text\": result['metadata']['text'], \"preprocessed\": result['metadata']['preprocessed']} for result in results['matches']]\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to retrieve relevant documents: {str(e)}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c49629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, context: List[Dict[str, Any]]) -> str:\n",
    "    '''Generate an answer using OpenAI's API with improved prompt engineering.'''\n",
    "    try:\n",
    "        context_str = \"\\n\".join([doc[\"text\"] for doc in context])\n",
    "        prompt = f'''You are a helpful AI assistant for a business. Use the following context to answer the user's question. \n",
    "        If the answer is not in the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context_str}\n",
    "\n",
    "        User Question: {query}\n",
    "\n",
    "        Please provide a concise and accurate answer based on the given context:'''\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant for a business.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to generate answer: {str(e)}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57152b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_bot(query: str) -> str:\n",
    "    '''Main function to handle user queries.'''\n",
    "    try:\n",
    "        relevant_docs = retrieve_relevant_docs(query)\n",
    "        answer = generate_answer(query, relevant_docs)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        logger.error(f'QA bot encountered an error: {str(e)}')\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again later.\"\n",
    "\n",
    "# Example usage and Gradio Interface\n",
    "documents = [\n",
    "    \"Our company, TechInnovate, was founded in 1995 by Dr. Jane Smith.\",\n",
    "    \"We specialize in AI-driven solutions for businesses, focusing on natural language processing and computer vision.\",\n",
    "    \"Our headquarters is located in San Francisco, California, with satellite offices in New York and London.\",\n",
    "    \"TechInnovate's flagship product, AIAssist, has been adopted by over 500 Fortune 1000 companies.\",\n",
    "    \"In 2022, we launched our new cloud-based AI platform, which has seen a 200% growth in user adoption within the first year.\",\n",
    "]\n",
    "\n",
    "# Add documents to the index\n",
    "add_documents_to_index(documents)\n",
    "\n",
    "# Gradio UI\n",
    "def gradio_interface(query):\n",
    "    return qa_bot(query)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"TechInnovate AI Assistant\",\n",
    "    description=\"Ask me anything about TechInnovate!\",\n",
    "    theme=\"default\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
